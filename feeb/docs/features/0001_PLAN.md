# Feature Plan: Allergen-Aware Ingredient API (OpenFoodFacts Integration)

## Overview

Build a standalone Python + FastAPI microservice that ingests OpenFoodFacts (OFF) taxonomy and product data into PostgreSQL, exposing REST endpoints for ingredient and allergen lookups. This service acts as the data layer for Feeb's allergen detection system, designed with extensibility to support custom ingredient data sources in future iterations.

---

## Database Schema

### New Tables to Create

All tables live in the existing PostgreSQL database (shared with the Hono backend). Schema migrations will be managed via Alembic.

**`ingredient`**
- `id` (SERIAL PRIMARY KEY)
- `code` (VARCHAR(255) UNIQUE NOT NULL) — OFF taxonomy code (e.g., `en:wheat-flour`)
- `name` (VARCHAR(500) NOT NULL) — display name
- `parent_code` (VARCHAR(255) NULLABLE) — hierarchical parent code
- `allergen_code` (VARCHAR(255) NULLABLE) — direct allergen link from taxonomy
- `source` (VARCHAR(50) NOT NULL DEFAULT 'off') — data provenance flag
- `last_updated` (TIMESTAMP DEFAULT NOW())
- INDEX on `code`, `name`, `source`

**`allergen`**
- `id` (SERIAL PRIMARY KEY)
- `code` (VARCHAR(255) UNIQUE NOT NULL) — OFF allergen code (e.g., `en:gluten`)
- `name` (VARCHAR(500) NOT NULL)
- `category` (VARCHAR(100) NULLABLE) — classification (e.g., "grain", "nut")
- `severity_level` (VARCHAR(50) NULLABLE) — reserved for future scoring
- INDEX on `code`, `name`

**`ingredient_allergen`** (many-to-many)
- `id` (SERIAL PRIMARY KEY)
- `ingredient_id` (INT FK → ingredient.id)
- `allergen_id` (INT FK → allergen.id)
- `certainty` (VARCHAR(50) DEFAULT 'direct') — "direct", "inferred", "possible"
- `source` (VARCHAR(50) NOT NULL DEFAULT 'off')
- UNIQUE constraint on `(ingredient_id, allergen_id, source)`

**`product`**
- `id` (SERIAL PRIMARY KEY)
- `barcode` (VARCHAR(50) UNIQUE NOT NULL) — EAN/UPC code
- `name` (VARCHAR(500) NOT NULL)
- `brand` (VARCHAR(255) NULLABLE)
- `lang` (VARCHAR(10) DEFAULT 'en')
- `created_at` (TIMESTAMP DEFAULT NOW())
- INDEX on `barcode`

**`product_ingredient`** (many-to-many)
- `id` (SERIAL PRIMARY KEY)
- `product_id` (INT FK → product.id)
- `ingredient_id` (INT FK → ingredient.id)
- `percent_estimate` (DECIMAL(5,2) NULLABLE) — percentage in product
- `rank` (INT NULLABLE) — position in ingredient list
- UNIQUE constraint on `(product_id, ingredient_id)`

**`product_allergen`** (denormalized for fast lookups)
- `id` (SERIAL PRIMARY KEY)
- `product_id` (INT FK → product.id)
- `allergen_id` (INT FK → allergen.id)
- `relation_type` (VARCHAR(50) NOT NULL) — "contains", "may_contain", "traces"
- `source` (VARCHAR(50) NOT NULL DEFAULT 'off')
- UNIQUE constraint on `(product_id, allergen_id, relation_type)`

---

## File Structure

Create a new directory at the project root: `/ingredient-api/`

```
ingredient-api/
├── alembic/                      # DB migration tool
│   ├── versions/
│   └── env.py
├── app/
│   ├── __init__.py
│   ├── main.py                   # FastAPI app entry point
│   ├── config.py                 # Load .env vars (DB_URL, OFF_BASE_URL, DATA_SOURCE)
│   ├── database.py               # SQLAlchemy async engine + session factory
│   ├── models.py                 # SQLAlchemy ORM models (Ingredient, Allergen, etc.)
│   ├── dal.py                    # Data access layer (get_ingredient_by_name, etc.)
│   └── routes.py                 # FastAPI route handlers
├── data_pipeline/
│   ├── __init__.py
│   ├── import_off.py             # Main import script with load_off_data()
│   ├── parsers.py                # OFF taxonomy + JSONL parsing logic
│   └── utils.py                  # Helper functions (download, decompress)
├── tests/
│   ├── test_dal.py
│   └── test_routes.py
├── .env.example
├── requirements.txt
├── alembic.ini
└── README.md
```

---

## Data Import Algorithm

### Entry Point: `data_pipeline/import_off.py`

```python
def load_off_data():
    """
    Downloads and imports OFF data into PostgreSQL.
    Can be invoked manually or via cron/scheduler.
    """
    1. Download files:
       - ingredients.txt → temp storage
       - allergens.txt → temp storage
       - en.openfoodfacts.org.products.jsonl.gz → temp storage
    
    2. Parse allergens.txt:
       - Read lines starting with "id:", "name:", "parents:"
       - Insert into `allergen` table (code, name, category derived from parents)
       - Build in-memory map: allergen_code → allergen_id
    
    3. Parse ingredients.txt:
       - Read lines starting with "id:", "name:", "parents:", "allergen:"
       - Insert into `ingredient` table (code, name, parent_code, allergen_code)
       - Build in-memory map: ingredient_code → ingredient_id
       - If allergen_code present, create entry in `ingredient_allergen` with certainty='direct'
    
    4. Parse products JSONL (sample only):
       - Decompress .gz file
       - For each product JSON:
         - Insert into `product` (barcode, name, brand, lang)
         - Parse ingredients_tags array → link to ingredient table
         - Insert into `product_ingredient` with rank
         - Parse allergens_tags array → link to allergen table
         - Insert into `product_allergen` with relation_type='contains'
       - Limit to first 10,000 products for MVP
    
    5. Set last_updated timestamp for all inserted records
    6. Commit transaction
    7. Log summary stats (counts inserted per table)
```

### Parsing Details: `data_pipeline/parsers.py`

**`parse_taxonomy_file(filepath: str) → List[Dict]`**
- OFF taxonomy files use indented key-value pairs
- Group consecutive lines into entries when "id:" is encountered
- Extract hierarchical relationships from "parents:" field
- Return list of dicts with keys: `code`, `name`, `parent_codes`, `allergen_code` (if present)

**`parse_product_jsonl(filepath: str, limit: int) → Iterator[Dict]`**
- Stream-read compressed JSONL file
- Yield parsed JSON objects up to `limit`
- Extract relevant fields: `code` (barcode), `product_name`, `brands`, `ingredients_tags`, `allergens_tags`

---

## API Endpoints

### File: `app/routes.py`

**`GET /ingredients/{name}`**
- Query: `?exact=true` (default: false for fuzzy search)
- Response:
  ```json
  {
    "ingredient": {
      "code": "en:wheat-flour",
      "name": "Wheat flour",
      "source": "off",
      "last_updated": "2025-10-07T12:00:00Z"
    },
    "allergens": [
      {
        "code": "en:gluten",
        "name": "Gluten",
        "certainty": "direct"
      }
    ]
  }
  ```
- Implementation: Call `dal.get_ingredient_by_name(name, exact)` → join with `ingredient_allergen` and `allergen` tables

**`GET /products/{barcode}`**
- Response:
  ```json
  {
    "product": {
      "barcode": "3017620422003",
      "name": "Nutella",
      "brand": "Ferrero"
    },
    "ingredients": [
      {"name": "Sugar", "rank": 1},
      {"name": "Palm oil", "rank": 2}
    ],
    "allergens": [
      {
        "name": "Milk",
        "relation_type": "contains"
      },
      {
        "name": "Nuts",
        "relation_type": "may_contain"
      }
    ]
  }
  ```
- Implementation: Call `dal.get_product_by_barcode(barcode)` → join with `product_ingredient`, `product_allergen`, `ingredient`, `allergen` tables

**`GET /health`**
- Returns `{"status": "ok", "db_connected": true}`

---

## Data Access Layer

### File: `app/dal.py`

All functions use SQLAlchemy async sessions and return Pydantic models (not raw ORM objects).

**`async def get_ingredient_by_name(name: str, exact: bool = False) → Optional[IngredientWithAllergens]`**
- If `exact=True`: Filter `ingredient.name = name`
- Else: Filter `ingredient.name ILIKE %name%` (case-insensitive partial match)
- Load related allergens via join on `ingredient_allergen`
- Return None if not found

**`async def get_product_by_barcode(barcode: str) → Optional[ProductWithDetails]`**
- Filter `product.barcode = barcode`
- Eager-load ingredients (via `product_ingredient`) sorted by rank
- Eager-load allergens (via `product_allergen`)
- Return None if not found

**`async def insert_allergen(code: str, name: str, category: Optional[str]) → int`**
- Insert or update (UPSERT on code)
- Return allergen_id

**`async def insert_ingredient(...) → int`**
- UPSERT on code
- Return ingredient_id

**`async def link_ingredient_allergen(ingredient_id: int, allergen_id: int, certainty: str) → None`**
- UPSERT on (ingredient_id, allergen_id, source='off')

---

## Configuration

### File: `app/config.py`

Load from `.env`:
- `DATABASE_URL` (PostgreSQL connection string, shared with Hono backend)
- `OFF_BASE_URL` (default: `https://world.openfoodfacts.org`)
- `DATA_SOURCE` (default: `off`)
- `SAMPLE_PRODUCT_LIMIT` (default: 10000)

Use `pydantic-settings` for validation.

---

## Dependencies

### File: `requirements.txt`

```
fastapi==0.110.0
uvicorn[standard]==0.27.0
sqlalchemy[asyncio]==2.0.28
asyncpg==0.29.0
alembic==1.13.1
pydantic-settings==2.2.0
httpx==0.27.0
python-dotenv==1.0.1
```

---

## Integration with Existing Hono Backend

The Hono backend (in `/server`) will call this FastAPI service via HTTP:
- Add environment variable `INGREDIENT_API_URL` to Hono's `.env` (e.g., `http://localhost:8000`)
- Create a new Hono service file `/server/src/services/ingredientApi.ts`:
  - `async function getIngredientByName(name: string): Promise<IngredientWithAllergens>`
  - Uses `fetch()` to call `GET /ingredients/{name}`
- Use this service in recipe upload handlers to auto-tag allergens

No changes to existing Hono files required for MVP; integration is additive.

---

## Execution Steps

1. **Initialize Project Structure**
   - Create `/ingredient-api/` directory
   - Generate files: `app/main.py`, `app/routes.py`, `app/dal.py`, `app/models.py`, `app/database.py`, `app/config.py`
   - Generate files: `data_pipeline/import_off.py`, `data_pipeline/parsers.py`, `data_pipeline/utils.py`
   - Create `.env.example`, `requirements.txt`, `alembic.ini`, `README.md`

2. **Database Setup**
   - Run `alembic init alembic`
   - Create migration: `alembic revision --autogenerate -m "Create ingredient allergen tables"`
   - Apply migration: `alembic upgrade head`

3. **Implement Data Pipeline**
   - In `import_off.py`: Implement `load_off_data()` with download, parse, insert logic
   - In `parsers.py`: Implement `parse_taxonomy_file()` and `parse_product_jsonl()`
   - In `utils.py`: Implement `download_file()` and `decompress_gz()`

4. **Implement API Layer**
   - In `models.py`: Define SQLAlchemy ORM models matching schema
   - In `database.py`: Configure async SQLAlchemy engine
   - In `dal.py`: Implement data access functions
   - In `routes.py`: Define FastAPI endpoints
   - In `main.py`: Mount routes, add CORS middleware, startup/shutdown hooks

5. **Run Data Import**
   - Execute: `python -m data_pipeline.import_off`
   - Verify data in PostgreSQL: `SELECT COUNT(*) FROM ingredient;`

6. **Test API**
   - Start server: `uvicorn app.main:app --reload`
   - Test: `curl http://localhost:8000/ingredients/wheat`
   - Test: `curl http://localhost:8000/products/3017620422003`

7. **Documentation**
   - Update `README.md` with setup instructions
   - Document API endpoints (auto-generated via FastAPI's `/docs`)

---

## Future Extensibility

- **Custom Data Source**: Add tables with `source='custom'`, implement separate importer in `data_pipeline/import_custom.py`
- **Automated Refresh**: Add cron job to re-run `load_off_data()` weekly
- **Hierarchical Allergen Inference**: Traverse ingredient parent hierarchy to infer allergens (e.g., "wheat flour" → "wheat" → "gluten")
- **Multilingual Support**: Store translations in separate `ingredient_name_i18n` table

---

## Non-Goals (Out of Scope for MVP)

- User authentication (API is internal-facing for now)
- Rate limiting / caching (add if performance becomes issue)
- Nutritional data parsing (OFF has this; defer to future feature)
- Multi-database support (PostgreSQL only)

